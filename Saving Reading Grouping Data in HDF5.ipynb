{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving, Reading, Grouping, and Manipulating with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change the directory \n",
    "os.chdir('E:\\BackUp\\__H2OResource\\MATLAB\\MattoGrasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Additional Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Random DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (10000,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write hdf5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\hdf5_data.h5','w') as hdf:\n",
    "               hdf.create_dataset('dataset1', data=matrix1)\n",
    "               hdf.create_dataset('dataset2', data=matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and Print HDF5 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\hdf5_data.h5','r') as hdf:\n",
    "    ls = list(hdf.keys())\n",
    "    print('List of datasets in this file: \\n', ls)\n",
    "    data = hdf.get('dataset1')\n",
    "    dataset1 = np.array(data)\n",
    "    print('Shape of dataset1: \\n', dataset1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Always important to close the file  \"f.close()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\hdf5_data.h5','r')\n",
    "ls = list(f.keys())\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.random.random(size = (1000,1000))\n",
    "matrix2 = np.random.random(size = (1000,1000))\n",
    "matrix3 = np.random.random(size = (1000,1000))\n",
    "matrix4 = np.random.random(size = (1000,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Datasets and Produce Groups and SubGroups inside hdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\hdf5_groups.h5', 'w') as hdf:\n",
    "    G1 = hdf.create_group('Group1')\n",
    "    G1.create_dataset('dataset1', data = matrix1)\n",
    "    G1.create_dataset('dataset4', data = matrix4)\n",
    "    \n",
    "    G2 = hdf.create_group('Group2/SubGroup1')\n",
    "    G2.create_dataset('dataset3', data = matrix3)\n",
    "    \n",
    "    G3 = hdf.create_group('Group2/SubGroup2')\n",
    "    G3.create_dataset('dataset2', data = matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\hdf5_groups.h5','r') as hdf:\n",
    "    base_items = list(hdf.items())\n",
    "    print('Items in the base directory:', base_items)\n",
    "    G1 = hdf.get('Group1')\n",
    "    G1_items = list(G1.items())\n",
    "    print('Items in Group1:', G1_items)\n",
    "    dataset4 = np.array(G1.get('dataset4'))\n",
    "    print(dataset4.shape)\n",
    "    G2 = hdf.get('Group2')\n",
    "    G2_items = list(G2.items())\n",
    "    print('Items in Group2:', G2_items)\n",
    "    G21 = G2.get('/Group2/SubGroup1')\n",
    "    G21_items = list(G21.items())\n",
    "    print('Items in Group21:', G21_items)\n",
    "    dataset3 = np.array(G21.get('dataset3'))\n",
    "    print(dataset3.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress HDF5 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\hdf5_groups_compressed.h5','w') as hdf:\n",
    "    G1 = hdf.create_group('Group1')\n",
    "    G1.create_dataset('dataset1', data = matrix1, compression=\"gzip\", compression_opts=9)\n",
    "    G1.create_dataset('dataset4', data = matrix4, compression=\"gzip\", compression_opts=9)\n",
    "    \n",
    "    G21 = hdf.create_group('Group2/SubGroup1')\n",
    "    G21.create_dataset('dataset3', data = matrix3, compression=\"gzip\", compression_opts=9)\n",
    "                       \n",
    "    G22 = hdf.create_group('Group2/SubGroup2')\n",
    "    G22.create_dataset('dataset2', data = matrix2, compression=\"gzip\", compression_opts=9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set and Read Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the HDF5 file\n",
    "hdf = h5py.File('E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\test.h5','w')\n",
    "\n",
    "# Create the datasets\n",
    "dataset1 = hdf.create_dataset('dataset1', data=matrix1)\n",
    "dataset2 = hdf.create_dataset('dataset2', data=matrix2)\n",
    "\n",
    "# Set Attributes\n",
    "dataset1.attrs['CLASS'] = 'DATA MATRIX'\n",
    "dataset1.attrs['VERSION'] = '1.1'\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the HDF5 file\n",
    "hdf = h5py.File('E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\test.h5','r')\n",
    "ls = list(hdf.keys())\n",
    "print('List of datasets in this file: \\n', ls)\n",
    "data = hdf.get('dataset1')\n",
    "dataset1 = np.array(data)\n",
    "print('Shape of dataset1: \\n', dataset1.shape)\n",
    "# read the attributes\n",
    "k = list(data.attrs.keys())\n",
    "v = list(data.attrs.values())\n",
    "# print the attributes\n",
    "print(k[0])\n",
    "print(v[0])\n",
    "print(data.attrs[k[0]])\n",
    "\n",
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another set of imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates (or opens in append mode) an hdf5 file\n",
    "hdf = pd.HDFStore('E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\hdf5_pandas.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Do not have the Dataset for this, it was unattainable \n",
    "#  df1 = pd.read_csv(''E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\FL_insurance_sample.csv')\n",
    "#  hdf.put('DF1', df1, format='table', data_columns=True)                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"city\": [\"Tripolo\", \"Sydney\", \"Tripoli\", \"Rome\", \"Rome\", \"Tripoli\", \"Rome\", \"Sydney\", \"Sydney\"],\n",
    "    \"rank\": [\"1st\", \"2nd\", \"1st\", \"2nd\", \"1st\", \"2nd\", \"1st\", \"2nd\", \"1st\"],\n",
    "    \"score1\": [44, 48, 39, 41, 38, 44, 34, 54, 61],\n",
    "    \"score2\": [67, 63, 55, 70, 64, 77, 45, 66, 72]\n",
    "}\n",
    "df2 = pd.DataFrame(data, columns = ['city','rank','score1','score2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.put('DF2Key', df2, format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.close()  #  do not forget to close the hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open hdf5 file for reading\n",
    "hdf = pd.HDFStore('E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\hdf5_pandas.h5', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2 = hdf.get('/DF2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(DF2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Image File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the HDF5 file\n",
    "hdf = h5py.File('E:\\\\BackUp\\\\__H2OResource\\\\MATLAB\\\\MattoGrasso\\\\MOD44B.A2000065.h12v10.006.2017081102216.hdf','r')\n",
    "#ls = list(hdf.keys())\n",
    "#print('List of datasets in this file: \\n', ls)\n",
    "#data = hdf.get('dataset1')\n",
    "#dataset1 = np.array(data)\n",
    "#print('Shape of dataset1: \\n', dataset1.shape)\n",
    "# read the attributes\n",
    "#k = list(data.attrs.keys())\n",
    "#v = list(data.attrs.values())\n",
    "# print the attributes\n",
    "#print(k[0])\n",
    "#print(v[0])\n",
    "#print(data.attrs[k[0]])\n",
    "\n",
    "#hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e99bb0637753a53091bd5e6e4065d5e03a4efa38d737c0e2f093c7e5db360448"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
